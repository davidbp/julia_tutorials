{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single layer nnet julia\n",
    "\n",
    "## Objective of the notebook\n",
    "\n",
    "The objective is to create a multilayer perceptron with a single hidden layer\n",
    "\n",
    "\n",
    "Code in python containing an example similar to this can be found here:\n",
    "\n",
    "http://cs231n.github.io/neural-networks-case-study/#linear\n",
    "\n",
    "## About the softmax function\n",
    "\n",
    "https://nolanbconaway.github.io/blog/2017/softmax-numpy\n",
    "\n",
    "### Toy NNets for education pourpouses in  Julia\n",
    "\n",
    "Interesting discussion how to make forward pass efficiently using BLAS:\n",
    "\n",
    "- https://discourse.julialang.org/t/blas-performance-issues-for-common-neural-network-patterns/565\n",
    "\n",
    "- http://int8.io/neural-networks-in-julia-hyperbolic-tangent-and-relu/\n",
    "\n",
    "- http://int8.io/backpropagation-from-scratch-in-julia-part-ii-derivation-and-implementation/\n",
    "\n",
    "- http://www.breloff.com/JuliaML-and-Plots/\n",
    "\n",
    "- https://github.com/JuliaQuant/OnlineAI.jl\n",
    "\n",
    "Many machine learning packages in Julia\n",
    "\n",
    "- https://github.com/svaksha/Julia.jl/blob/master/AI.md#hmm\n",
    "\n",
    "#### Add the follwoing packages before executing this notebok\n",
    "\n",
    "- Pkg.add(\"MLDatautils\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using LossFunctions\n",
    "using BenchmarkTools\n",
    "#Pkg.add(\"LossFunctions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using MNIST\n",
    "#train = MNIST.traindata()\n",
    "#T = Float32\n",
    "#X_train = Array{T}(train[1]);\n",
    "#y_train = Vector{Int32}(train[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "train = MLDatasets.MNIST.traindata()\n",
    "T = Float32\n",
    "X_train = Array{T}(reshape(train[1], 28*28, 60000))\n",
    "y_train = Vector{Int32}(train[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 RowVector{Int32,Array{Int32,1}}:\n",
       " 0  1  2  3  4  5  6  7  8  9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort(unique(y_train))'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Linear layer and relu layer\n",
    "\n",
    "Play with functions `A_mult_Bt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_visible = 784\n",
    "n_hidden = 500\n",
    "batch_size = 200\n",
    "\n",
    "srand(1234)\n",
    "W1 = randn(T, n_hidden, n_visible );\n",
    "W1 = W1/norm(W1)\n",
    "b = zeros(n_hidden);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(X_train[:,1:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#linear layer\n",
    "batch = W1 * X_train[:,1:3].+b\n",
    "\n",
    "#relu\n",
    "l1_batch = batch .* (batch .>0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp.(batch)./sum(exp.(batch),1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Array{Float64,2}:\n",
       " 545.813  551.224  535.861"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(exp.(l1_batch),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.(l1_batch)./sum(exp.(l1_batch),1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing linear layer: Investigate time of sparse matrix\n",
    "\n",
    "- http://stackoverflow.com/questions/36673939/updating-a-dense-vector-by-a-sparse-vector-in-julia-is-slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_batch  = sparse(X_train[:,1:5000]);\n",
    "sp_b = sparsevec(b);\n",
    "sp_W1 = sparse(W1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.333627 seconds (386.18 k allocations: 109.565 MiB, 1.71% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time sp_W1 * sp_batch .+ sp_b;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.077175 seconds (61 allocations: 43.566 MiB, 13.23% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time W1 * X_train[:,1:5000] .+ b;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  43.57 MiB\n",
       "  allocs estimate:  58\n",
       "  --------------\n",
       "  minimum time:     47.559 ms (6.75% GC)\n",
       "  median time:      51.241 ms (12.14% GC)\n",
       "  mean time:        57.046 ms (10.31% GC)\n",
       "  maximum time:     89.681 ms (7.97% GC)\n",
       "  --------------\n",
       "  samples:          88\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark (BLAS.gemm('N','N', T(1.0), W1, X_train[:,1:5000]) .+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  28.61 MiB\n",
       "  allocs estimate:  89\n",
       "  --------------\n",
       "  minimum time:     44.619 ms (4.80% GC)\n",
       "  median time:      52.091 ms (8.32% GC)\n",
       "  mean time:        55.400 ms (7.85% GC)\n",
       "  maximum time:     85.340 ms (6.22% GC)\n",
       "  --------------\n",
       "  samples:          91\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark (W1*view(X_train,:,1:5000) .+ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  9.54 MiB\n",
       "  allocs estimate:  39\n",
       "  --------------\n",
       "  minimum time:     37.344 ms (0.00% GC)\n",
       "  median time:      49.433 ms (0.00% GC)\n",
       "  mean time:        50.645 ms (3.85% GC)\n",
       "  maximum time:     72.884 ms (8.01% GC)\n",
       "  --------------\n",
       "  samples:          99\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We could include the bias in W1 and append a row or col in X_train\n",
    "# full of ones so that the affine transformation is done in a single\n",
    "# matrix multiplication call\n",
    "@benchmark (BLAS.gemm('N','N', T(1.0), W1, view(X_train,:,1:5000)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the code that  **`W1*view(X_train,:,1:5000) .+ b`** calls using the `@code_lowered` macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeInfo(:(begin \n",
       "        nothing # line 346:\n",
       "        (Base.depwarn)((Base.string)(\".+\", \" is no longer a function object; use broadcast(\", Base.+, \", ...) instead\"), :.+) # line 348:\n",
       "        return (Base.broadcast)(Base.+, a, b)\n",
       "    end))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@code_lowered (W1*view(X_train,:,1:5000) .+ b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract type NeuralNetLayer end\n",
    "\n",
    "type LinearLayer{T} <: NeuralNetLayer\n",
    "    \"\"\"\n",
    "    Standard layer between activations.\n",
    "    The output of this layer for a given input is meant to be a matrix product \n",
    "    of the input times W\n",
    "    \"\"\"\n",
    "    input_dim::Int\n",
    "    output_dim::Int\n",
    "    W::Array{T}\n",
    "    b::Vector{T}\n",
    "    \n",
    "    #grad_W::Array{T}\n",
    "    #grad_b::Vector{T}\n",
    "\n",
    "    seed::Int\n",
    "end\n",
    "\n",
    "function LinearLayer{T}(input, output; seed=1234) where T<:Any\n",
    "    srand(seed)\n",
    "    return LinearLayer(input,\n",
    "                       output,\n",
    "                       randn(T,output,input)/sqrt(input),\n",
    "                       zeros(output), \n",
    "                       seed)\n",
    "end\n",
    "\n",
    "Base.show(io::IO, l::LinearLayer{T}) where T <: Number = \\,\n",
    "    print(io,\"LinearLayer{$T} [input_dim: $(l.input_dim), ouput_dim: $(l.output_dim)]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearLayer{Float64} [input_dim: 784, ouput_dim: 500]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 784\n",
    "output_dim = 500\n",
    "l = LinearLayer{Float32}(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract type ActivationFunction end\n",
    "\n",
    "type SigmoidActivation{T} <: ActivationFunction\n",
    "    \"\"\"\n",
    "    Relu Activation function latyer\n",
    "    \"\"\"\n",
    "    dim::Int\n",
    "end\n",
    "\n",
    "type ReluActivation{T}  <: ActivationFunction\n",
    "    \"\"\"\n",
    "    Relu Activation function latyer\n",
    "    \"\"\"\n",
    "    dim::Int\n",
    "end\n",
    "\n",
    "type SoftMaxActivation{T}  <: ActivationFunction\n",
    "    \"\"\"\n",
    "    Standard Sotmax Activation function\n",
    "    The output of this layer for a given input is meant to be \n",
    "        output_k = e^(X)_k /sum_j (X)_j\n",
    "    \"\"\"\n",
    "    dim::Int\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type SoftMaxLayer{T} <: ActivationFunction\n",
    "    \"\"\"\n",
    "    Standard Sotmax layer.\n",
    "    The output of this layer for a given input is meant to be \n",
    "        \n",
    "        output_k = e^(W*X + b)_k /sum_j (W*X + b)_j\n",
    "    \n",
    "    \"\"\"\n",
    "    input_dim::Int\n",
    "    output_dim::Int\n",
    "    W::Array{T}\n",
    "    \n",
    "    seed::Int\n",
    "end\n",
    "\n",
    "function SoftMaxLayer{T}(input, output; seed=1234) where T<:Number\n",
    "    srand(seed)\n",
    "    return SoftMaxLayer(input,\n",
    "                        output,\n",
    "                        randn(T, output, input)/sqrt(input),\n",
    "                        #  zeros(T, output, input),\n",
    "                        seed)\n",
    "end\n",
    "\n",
    "Base.show(io::IO, l::SoftMaxLayer{T}) where T <: Number = \\,\n",
    "print(io,\"SoftMaxLayer{$T} [input_dim: $(l.input_dim), ouput_dim: $(l.output_dim)]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "About Softmax layer\n",
    "\n",
    "http://stats.stackexchange.com/questions/79454/softmax-layer-in-a-neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First example\n",
    "\n",
    "Now let us define the structure, weight types (float type) of a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 500\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_classifer = [LinearLayer{Float32}(input_dim , hidden_dim),\n",
    "                 ReluActivation{Float32}(hidden_dim),\n",
    "                 SoftMaxLayer{Float32}(hidden_dim, output_dim)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearLayer{Float64} [input_dim: 784, ouput_dim: 500]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReluActivation{Float32}(500)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifer[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftMaxLayer{Float64} [input_dim: 500, ouput_dim: 10]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifer[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with the network\n",
    "\n",
    "We have defined a MLP as list of layers and activation functions.\n",
    "\n",
    "In order to make a prediction we need to make a forward pass through the network.\n",
    "Let us assume by now that we have a good set of weights at each layer in the network and\n",
    "we want to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 7 methods)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward(layer::LinearLayer, Xbatch::Array)\n",
    "    \"\"\"\n",
    "    Given an input batch where the data comes as columns this method propagates \n",
    "    the batch using the weights of the linear layer\n",
    "    \"\"\"\n",
    "    return layer.W * Xbatch .+ layer.b\n",
    "end\n",
    "\n",
    "function forward(layer::ReluActivation, Xbatch::Array)\n",
    "    return Xbatch.*( Xbatch .> 0.)\n",
    "end\n",
    "\n",
    "function forward(layer::SigmoidActivation, Xbatch::Array)\n",
    "    return 1./( 1 .+ exp.(-Xbatch))\n",
    "end\n",
    "\n",
    "function forward(layer::SoftMaxActivation, Xbatch::Array)\n",
    "    \"\"\"\n",
    "    Layer shrinking the output to [0,1] values.\n",
    "    Notice that sum(exp(Xbatch),1) will generate a Matrix with as many elements as\n",
    "    columns in Xbatch. \n",
    "    \"\"\"\n",
    "    return exp.(Xbatch)./sum(exp.(Xbatch), 1)\n",
    "end\n",
    "\n",
    "function forward(layer::SoftMaxLayer, Xbatch::Array)\n",
    "    \"\"\"\n",
    "    Layer shrinking the output to [0,1] values.\n",
    "    Notice that sum(exp(Xbatch),1) will generate a Matrix with as many elements as\n",
    "    columns in Xbatch. \n",
    "    \"\"\"\n",
    "    Xbatch_out = softmax_layer.W * Xbatch\n",
    "    return exp.(Xbatch_out)./sum(exp.(Xbatch_out), 1)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xbatch = X_train[:,1:25];\n",
    "ybatch = y_train[1:25];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 25)"
     ]
    }
   ],
   "source": [
    "aux = forward(mlp_classifer[1], Xbatch)\n",
    "print(size(aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 25)(500, 25)(10, 25)"
     ]
    }
   ],
   "source": [
    "aux = forward(mlp_classifer[1], Xbatch)\n",
    "print(size(aux))\n",
    "aux = forward(mlp_classifer[2], aux)\n",
    "print(size(aux))\n",
    "aux = forward(mlp_classifer[3], aux)\n",
    "print(size(aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict_proba (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict_proba(mlp, Xbatch::Array)\n",
    "    for l in mlp\n",
    "        Xbatch = forward(l, Xbatch)\n",
    "    end\n",
    "    return Xbatch\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.608586 seconds (938.58 k allocations: 49.863 MiB, 3.18% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×25 Array{Float64,2}:\n",
       " 0.110592   0.190445   0.0911891  …  0.0980983  0.104619   0.0916908\n",
       " 0.099009   0.109728   0.0970957     0.119944   0.105477   0.0814854\n",
       " 0.0734697  0.0690236  0.0826141     0.066948   0.0570211  0.0682049\n",
       " 0.117364   0.103467   0.0962567     0.0986178  0.12321    0.104741 \n",
       " 0.0802728  0.0745412  0.0905434     0.109321   0.101395   0.100484 \n",
       " 0.115514   0.118488   0.120487   …  0.123378   0.110909   0.0904144\n",
       " 0.124602   0.0953256  0.131559      0.122778   0.111915   0.147542 \n",
       " 0.0772554  0.0546428  0.0759803     0.0694765  0.121296   0.0858057\n",
       " 0.0728219  0.0805403  0.090678      0.10233    0.0939435  0.107779 \n",
       " 0.129099   0.103798   0.123596      0.0891084  0.0702147  0.121853 "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each column contains a vector that represents\n",
    "# The conditional probability of the target beein from a particular class having observed\n",
    "# the input vector.\n",
    "\n",
    "@time predict_proba(mlp_classifer, Xbatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Encoding class values as \"onehot\" vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_hot_encoding (generic function with 1 method)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function one_hot_encoding(y_train::Vector,\n",
    "                          unique_classes::Vector,\n",
    "                          class_to_pos::Dict)\n",
    "    \n",
    "    encoded_classes = zeros(length(unique_classes), length(y_train))\n",
    "    for (i,y) in enumerate(y_train)\n",
    "        encoded_classes[class_to_pos[y],i] = 1\n",
    "    end\n",
    "    return encoded_classes\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_classes = sort(unique(y_train))\n",
    "class_to_pos = Dict(class => pos for (pos,class) in enumerate(unique_classes));    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "class integer: Int32[5, 0, 4]\n",
      "Encoding:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×3 Array{Float64,2}:\n",
       " 0.0  1.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  1.0\n",
       " 1.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nclass integer: \", y_train[1:3])\n",
    "print(\"\\nEncoding:\\n\")\n",
    "one_hot_encoding(y_train[1:3], unique_classes, class_to_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×25 Array{Float64,2}:\n",
       " 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  1.0\n",
       " 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  1.0  0.0  0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(ybatch, unique_classes, class_to_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "\n",
    "\n",
    "## Cross entropy loss for mlp_classifier\n",
    "- http://neuralnetworksanddeeplearning.com/chap3.html\n",
    "- http://datascience.stackexchange.com/questions/9302/the-cross-entropy-error-function-in-neural-networks\n",
    "\n",
    "We will focus now on an standard loss function for classification problems. The cross entropy loss.\n",
    "\n",
    "$$\n",
    "    \\text{loss}\\left(h(x), e(t) \\right) = - \\sum_{i=1}^{C} e(t)_i \\log(h(x)_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type crossentropy_loss\n",
    "    dim::Int\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossentropy_loss(10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossentropy = crossentropy_loss(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossentropy.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 6 methods)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward(loss::crossentropy_loss, Y_hat_batch::Matrix, Y_encoded::Matrix)\n",
    "    \"\"\"\n",
    "    Should this function do the onehot encoding?\n",
    "    In order to save memory it seems reasonable but...\n",
    "\n",
    "    C=number of classes\n",
    "    e(t) = Encoding (as vector) of class t\n",
    "      \n",
    "          loss (h(x), e(t)) = \\sum_{i=1}^{C} e(t)_i  log (h(x)_i)\n",
    "\n",
    "    Returns the loss between the batch\n",
    "    \"\"\"\n",
    "    # This breaks if Y_hat_batch contains 0 because log(0)= -Inf\n",
    "    # return - sum(Y_encoded.*log.(Y_hat_batch))\n",
    "    \n",
    "    n_samples = size(Y_encoded, 2)\n",
    "    cross_entropy = 0.\n",
    "    \n",
    "    @inbounds for m in 1:n_samples\n",
    "        for d in 1:crossentropy.dim\n",
    "            cross_entropy +=  Y_encoded[d,m] * log(Y_hat_batch[d,m])\n",
    "        end\n",
    "    end\n",
    "    return -cross_entropy\n",
    "end\n",
    "\n",
    "function forward(loss::crossentropy_loss, y_hat::Vector, y::Vector)\n",
    "    \"\"\"\n",
    "    Should this function do the onehot encoding?\n",
    "    In order to save memory it seems reasonable but...\n",
    "    C = number of classes\n",
    "    e(t) = Encoding (as vector) of class t\n",
    "          loss (h(x), e(t)) = \\sum_{i=1}^{C} e(t)_i  log (h(x)_i)\n",
    "    \n",
    "    Returns the loss between the batch\n",
    "    \"\"\"\n",
    "    # This breaks if Y_hat_batch contains 0 because log(0)= -Inf\n",
    "    # return - sum(Y_encoded.*log.(Y_hat_batch))\n",
    "    cross_entropy = 0.\n",
    "\n",
    "    @inbounds for d in 1:length(y)\n",
    "        cross_entropy +=  log(y_hat[d])\n",
    "    end\n",
    "    return -cross_entropy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 25)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(predict_proba(mlp_classifer, Xbatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.796679057341976"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(crossentropy, \n",
    "        predict_proba(mlp_classifer, Xbatch),\n",
    "        one_hot_encoding(ybatch, unique_classes, class_to_pos)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.218675162901754"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(crossentropy, \n",
    "        predict_proba(mlp_classifer, Xbatch)[:,1:3],\n",
    "        one_hot_encoding(ybatch, unique_classes, class_to_pos)[:,1:3]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = Array([1.; 0.; 0.; 0.])\n",
    "y_hat = Array([0.9; 0.1; 0.; 0.]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 0.0, 0.0, 0.0], [0.9, 0.1, 0.0, 0.0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inf"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why is this inf??\n",
    "forward(crossentropy, y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000005 seconds (5 allocations: 176 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Inf"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time forward(crossentropy, y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing crossentropy onehot vs unique component implementations\n",
    "\n",
    "https://jamesmccaffrey.wordpress.com\n",
    "\n",
    "- Let $h(x^m)$ be the probability of the different classes for a given input $x^m$. \n",
    "- Let $y^m$ be an integer containing the class label for $x^m$. \n",
    "\n",
    "Then the crossentropy for the pair $(h(x^m), e(y^m))$ is defined as:\n",
    "\n",
    "$$\n",
    "    \\text{cross_entropy}\\left(h(x^m), e(y^m) \\right) = - \\sum_{i=1}^{C} e(y^m)_i \\log \\left( h(x^m)_i\\right) = y^m \\log \\left(h(x^m)_{y^m}\\right)\n",
    "$$\n",
    "\n",
    "Be carefull, the cross_entropy loss is not symmetric.\n",
    "\n",
    "\n",
    "#### Example showing the importance of the crossentropy\n",
    "\n",
    "The crossentropy loss measures how good the probabilities of the classes are, not just the final prediction.\n",
    "\n",
    "```\n",
    "computed       | targets              | correct?\n",
    "-----------------------------------------------\n",
    "0.3  0.3  0.4  | 0  0  1 (democrat)   | yes\n",
    "0.3  0.4  0.3  | 0  1  0 (republican) | yes\n",
    "0.1  0.2  0.7  | 1  0  0 (other)      | no\n",
    "```\n",
    "\n",
    "This neural network has classification error of 1/3 = 0.33, or equivalently a classification accuracy of 2/3 = 0.67. Notice that the NN just barely gets the first two training items correct and is way off on the third training item. But now consider the following neural network:\n",
    "\n",
    "```\n",
    "computed       | targets              | correct?\n",
    "-----------------------------------------------\n",
    "0.1  0.2  0.7  | 0  0  1 (democrat)   | yes\n",
    "0.1  0.7  0.2  | 0  1  0 (republican) | yes\n",
    "0.3  0.4  0.3  | 1  0  0 (other)      | no\n",
    "```\n",
    "\n",
    "This NN also has a classification error of 1/3 = 0.33. But this second NN is better than the first because it nails the first two training items and just barely misses the third training item. To summarize, classification error is a very crude measure of error.\n",
    "\n",
    "Now consider cross-entropy error. The cross-entropy error for the first training item in the first neural network above is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 0.0  0.0  1.0\n",
       " 0.0  0.1  0.0\n",
       " 1.0  0.0  0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = Array([ [0.3  0.3  0.4]; [0.3  0.4  0.3]; [0.1  0.2  0.7]])'\n",
    "y_true = Array([ [0.   0.   1. ]; [0.0  0.1  0.0]; [1.0  0.0  0.0]])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916290731874155"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-( (log(0.3)*0) + (log(0.3)*0) + (log(0.4)*1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the case of neural network classification, the computation is a bit odd because all terms but one will go away. (There are several good explanations of how to compute cross-entropy on the Internet.) \n",
    "\n",
    "The average cross-entropy error (ACE) for the first neural network is computed as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3783888522474517"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(log(0.4) + log(0.4) + log(0.1)) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average cross-entropy error for the second neural network is: 0.639, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6391075640678003"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(log(0.7) + log(0.7) + log(0.3)) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the average cross-entropy error for the second, superior neural network is smaller than the ACE error for the first neural network. The ln() function in cross-entropy takes into account the closeness of a prediction and is a more granular way to compute error.\n",
    "\n",
    "\n",
    "By the way, you can also measure neural network quality by using mean squared error but this has problems too. The squared error term for the first item in the first neural network would be:\n",
    "\n",
    "```\n",
    "(0.3 - 0)^2 + (0.3 - 0)^2 + (0.4 - 1)^2 = 0.09 + 0.09 + 0.36 = 0.54\n",
    "```\n",
    "\n",
    "And so the mean squared error for the first neural network is:\n",
    "\n",
    "```\n",
    "(0.54 + 0.54 + 1.34) / 3 = 0.81\n",
    "```\n",
    "The mean squared error for the second, better, neural network is:\n",
    "\n",
    "```\n",
    "(0.14 + 0.14 + 0.74) / 3 = 0.34\n",
    "```\n",
    "\n",
    "MSE isn’t a hideously bad approach but if you think about how MSE is computed you’ll see that, compared to ACE, MSE gives too much emphasis to the incorrect outputs. It might also be possible to compute a modified MSE that uses only the values associated with the 1s in the target, but I have never seen that approach used or discussed.\n",
    "\n",
    "So, I think this example explains why using cross-entropy error is clearly preferable to using classification error. Somewhat unfortunately there are some additional issues here. The discussion above refers to computing error during the training process. After training, to get an estimate of the effectiveness of the neural network, classification error is usually preferable to MSE or ACE. The idea is that classification error is ultimately what you’re interested in.\n",
    "\n",
    "Suppose you are using back-propagation for training. The back-propagation algorithm computes gradient values which are derived from some implicit measure of error. Typically the implicit error is mean squared error, which gives a particular gradient equation that involves the calculus derivative of the softmax output activation function. But you can use implicit cross-entropy error instead of implicit mean squared error. This approach changes the back-propagation equation for the gradients. I have never seen research which directly addresses the question of whether to use cross-entropy error for both the implicit training measure of error and also neural network quality evaluation, or to use cross-entropy just for quality evaluation. Such research may (and fact, probably) exists, but I’ve been unable to track any papers down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_entropy (generic function with 2 methods)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cross_entropy(prob_Y_given_X::Matrix, Y_encoded::Matrix)\n",
    "    cross_entropy = 0.  \n",
    "    @inbounds for m in 1: size(Y_encoded,2)\n",
    "        for d in 1:size(Y_encoded,1)\n",
    "            cross_entropy +=  Y_encoded[d,m] * log(prob_Y_given_X[d,m])\n",
    "        end\n",
    "    end\n",
    "    return -cross_entropy\n",
    "end\n",
    "\n",
    "function cross_entropy(prob_Y_given_X::Matrix, y::Vector)\n",
    "    cross_entropy = 0.\n",
    "    @simd for m in 1:length(y)\n",
    "        cross_entropy +=  log(prob_Y_given_X[y[m],m])\n",
    "    end\n",
    "    return -cross_entropy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossentropy: 4.135166556742355\n",
      "crossentropy: 4.135166556742355\n",
      "crossentropy: 4.135166556742355\n",
      "crossentropy: 4.135166556742355\n"
     ]
    }
   ],
   "source": [
    "y_pred     = Array([ [0.3  0.3  0.4]; [0.3  0.4  0.3]; [0.1  0.2  0.7]])'\n",
    "y_true_enc = Array([ [0.   0.   1. ]; [0.0  1.0  0.0]; [1.0  0.0  0.0]])'\n",
    "y_true     = Array([ 3, 2, 1])\n",
    "\n",
    "println(\"crossentropy: \",  cross_entropy(y_pred , y_true_enc))\n",
    "println(\"crossentropy: \", -(log(0.4) + log(0.4) + log(0.1)))\n",
    "println(\"crossentropy: \", -(log(y_pred[y_true[1],1]) + log(y_pred[y_true[2],2]) + log(y_pred[y_true[3],3])))\n",
    "println(\"crossentropy: \",  cross_entropy(y_pred , y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO: Computing Gradients \n",
    "\n",
    "### Le us compute the gradient of the loss for a given input vector\n",
    "\n",
    "Now we will deal with the learning part. That is, given a MLP architecture we will tune the weights in order to minimize some error function. \n",
    "\n",
    "- Let $z^L$ be the preactivation at layer $L$.\n",
    "- Let $h(x)$ be the output values of the network.\n",
    "- Let $e(y)$ be the onehot encoding of class $y$.\n",
    "\n",
    "\n",
    "####  Computing $\\delta^L$ if the loss function is the crossentropy and the output layer is a softmax\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^L = \\nabla_{{z^{\\,L}\\,\\,\\,}}  loss( h(x), e(y) ) = (h(x) - e(y))\n",
    "\\end{equation}\n",
    "\n",
    "#### Computing $\\delta^l$ using $\\delta^{l+1}$ for any $1 \\leq l<L$ \n",
    "\n",
    "$$\n",
    "\\delta^l = \\big(W^{l+1 \\,\\,} \\big)^{\\,T}  \\delta^{l+1} .* g'(z^l)\n",
    "$$\n",
    "\n",
    "#### Computing  gradient of the weignts at every layer using $\\delta^l$ and $a^{l-1}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla_{W^l} = \\big( a^{l-1\\,\\,} \\big)^{\\,T}  \\delta^l \n",
    "$$\n",
    "\n",
    "#### Computing  gradient of the biases at every layer using $\\delta^l$ and $a^{l-1}$\n",
    "$$\n",
    "\\nabla_{b^l} =  \\delta^l \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Hinton matlab code\n",
    "\n",
    "\n",
    "    %%% Error back-propagation\n",
    "    df = [];\n",
    "\n",
    "    Ix = IO;\n",
    "    \n",
    "    %%% do not use outputLayer{nHiddenLayers}: nHiddenLayers may be 0\n",
    "    dw = outputHiddenLayers' * Ix; \n",
    "    df{nHiddenLayers+1} = dw;\n",
    "\n",
    "    for nLayer=nHiddenLayers:-1:1\n",
    "      Ix = (Ix * Weights{nLayer+1}') .* MLE_MultilayerPerceptron_DerivativeFactor(ActFunHiddens{nLayer},\n",
    "                                          outputLayer{nLayer});\n",
    "      \n",
    "      %%% removes the constant column (the added ones for the bias)\n",
    "      Ix = Ix(:,1:end-1);   \n",
    "      if nLayer > 1\n",
    "        dw = outputLayer{nLayer-1}' * Ix; \n",
    "      else\n",
    "        dw = Data' * Ix;                  \n",
    "      end;\n",
    "      df{nLayer} = dw;\n",
    "    end;\n",
    "    \n",
    "    \n",
    "#### derivatives activations\n",
    " \n",
    " The old MATLAB hinton's code look like\n",
    "\n",
    "\n",
    "    function DerivativeFactor = MLE_MultilayerPerceptron_DerivativeFactor(ActFun,outputLayerAct);\n",
    "\n",
    "    if strcmp(ActFun,'tanhyper')\n",
    "      DerivativeFactor = 1 - outputLayerAct .* outputLayerAct;\n",
    "    elseif strcmp(ActFun,'logistic')\n",
    "      DerivativeFactor = outputLayerAct .* (1 - outputLayerAct);\n",
    "    elseif strcmp(ActFun,'hardtanhyper')\n",
    "      DerivativeFactor = ones(size(outputLayerAct));     %%% set to 0 if outputLayerAct<-1 or outputLayerAct>+1\n",
    "      DerivativeFactor = DerivativeFactor .* (outputLayerAct > -1) .* (outputLayerAct < +1);\n",
    "    elseif strcmp(ActFun,'reclinear')\n",
    "      DerivativeFactor = (outputLayerAct > 0);\n",
    "    elseif strcmp(ActFun,'softreclinear')                %%% softplus\n",
    "      DerivativeFactor = 1-exp(-outputLayerAct);         %%% y = log(1+e^x) => dy = 1/(1+e^{-x}) = 1-e^{-y}\n",
    "    elseif strcmp(ActFun,'linear')\n",
    "      DerivativeFactor = 1;\n",
    "    elseif strcmp(ActFun,'sine')\n",
    "      DerivativeFactor = +sqrt( 1 - outputLayerAct.^2 ); %%% sine/cosine: we may lose the sign (we would need Data*Weights)???\n",
    "    elseif strcmp(ActFun,'cosine')\n",
    "      DerivativeFactor = -sqrt( 1 - outputLayerAct.^2 ); %%% sine/cosine: we may lose the sign (we would need Data*Weights)???\n",
    "    else error('MLE_MultilayerPerceptron_DerivativeFactor: ActFun not implemented');\n",
    "    end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_classifer = [LinearLayer{Float32}(input_dim, hidden_dim),\n",
    "                 SigmoidActivation{Float32}(hidden_dim),\n",
    "                 SoftMaxLayer{Float32}(hidden_dim, output_dim)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_classifer = [LinearLayer{Float32}(input_dim, hidden_dim),\n",
    "                 SigmoidActivation{Float32}(hidden_dim),\n",
    "                 LinearLayer{Float32}(hidden_dim, output_dim),\n",
    "                 SoftMaxActivation{Float32}(output_dim)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delta term for the output layer for (softmax, crossentropy loss)\n",
    "\n",
    "Since the delta terms are the gradients of the loss with respect to the preactivation at some layer, the gradient wil depend on both the softmax and the loss function. \n",
    "\n",
    "**If the softmax output uses sigmoid activations the delta term has the exact same form**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 6 methods)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backward(softmax::SoftMaxLayer,\n",
    "                  loss::crossentropy_loss,\n",
    "                  A_batch::Matrix, \n",
    "                  Y_enc_batch::Matrix)\n",
    "    \"\"\"\n",
    "    Returns the Gradient of the crossentropy loss with respect to the preactivation\n",
    "    This is usualy called the error at the output layer.\n",
    "    \"\"\"\n",
    "    return softmax.W' * (A_batch - Y_enc_batch)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossentropy_loss(10)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch_enc = one_hot_encoding(y_train[1:25], unique_classes, class_to_pos);\n",
    "ce_loss_10_classes = crossentropy_loss(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "act1 = forward(mlp_classifer[1], Xbatch)\n",
    "act2 = forward(mlp_classifer[2], act1)\n",
    "act3 = forward(mlp_classifer[3], act2);\n",
    "act4 = forward(mlp_classifer[4], act3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 25)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(act3 - y_batch_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 7 methods)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backward(softmax::SoftMaxActivation,\n",
    "                  loss::crossentropy_loss,\n",
    "                  A_batch::Matrix, \n",
    "                  Y_enc_batch::Matrix)\n",
    "    \"\"\"\n",
    "    Returns the Gradient of the crossentropy loss with respect to the preactivation\n",
    "    This is usualy called the error at the output layer.\n",
    "    \"\"\"\n",
    "    return (A_batch - Y_enc_batch)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_4 = backward(mlp_classifer[4], ce_loss_10_classes, act3, y_batch_enc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 10)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size((mlp_classifer[3].W'*err_4) * act3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delta term for the hidden layers\n",
    "\n",
    "      \n",
    "    def bprop(self, delta_in):\n",
    "        x_t = np.transpose(self.x)\n",
    "        self.grad_W = np.dot(x_t, delta_in)\n",
    "        self.grad_b = delta_in.sum(axis=0)\n",
    "        W_T = np.transpose(self.W)\n",
    "        self.delta_out = np.dot(delta_in,W_T)\n",
    "        return self.delta_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 6 methods)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 ways to write the backward mehotd for a SigmoidActivation\n",
    "\n",
    "# This needs the layer to store the activations during the forward pass\n",
    "function backward(layer::SigmoidActivation, delta_in)\n",
    "    delta_out = layer.act * (1 - layer.act) * delta_in\n",
    "    return delta_out\n",
    "end\n",
    "\n",
    "# This DOES NOT need the layer to store the activations during the forward pass\n",
    "function backward(layer::SigmoidActivation, delta_in, activation_batch)\n",
    "    delta_out = activation_batch .* (1 .- activation_batch) .* delta_in\n",
    "    return delta_out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 25), (500, 25))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(err_3), size(act2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mDimensionMismatch(\"A has dimensions (500,25) but B has dimensions (500,25)\")\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mDimensionMismatch(\"A has dimensions (500,25) but B has dimensions (500,25)\")\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mgemm_wrapper!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,2}, ::Char, ::Char, ::Array{Float64,2}, ::Array{Float64,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./linalg/matmul.jl:345\u001b[22m\u001b[22m",
      " [2] \u001b[1m*\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,2}, ::Array{Float64,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./linalg/matmul.jl:146\u001b[22m\u001b[22m",
      " [3] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "act2 * err_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SigmoidActivation{Float32}(500)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifer[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mDimensionMismatch(\"A has dimensions (500,25) but B has dimensions (500,25)\")\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mDimensionMismatch(\"A has dimensions (500,25) but B has dimensions (500,25)\")\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mgemm_wrapper!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,2}, ::Char, ::Char, ::Array{Float64,2}, ::Array{Float64,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./linalg/matmul.jl:345\u001b[22m\u001b[22m",
      " [2] \u001b[1m*\u001b[22m\u001b[22m at \u001b[1m./operators.jl:424\u001b[22m\u001b[22m [inlined]",
      " [3] \u001b[1mbackward\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::SigmoidActivation{Float32}, ::Array{Float64,2}, ::Array{Float64,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[166]:11\u001b[22m\u001b[22m",
      " [4] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "backward(mlp_classifer[2], err_3, act1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 6 methods)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 ways to write the backward mehotd for a  Liner Layer\n",
    "\n",
    "function backward(layer::LinearLayer, delta_in, activation_batch)\n",
    "    delta_out =  dela_in * a'\n",
    "\n",
    "    \n",
    "    return delta_out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "back_3 = backward(mlp_classifer[3], ce_loss_10_classes, act3, y_batch_enc)\n",
    "back_2 = backward(mlp_classifer[2],  act2, y_batch_enc)\n",
    "back_1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 500), (10, 500), (10, 500))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient of the grad_W of the softmax\n",
    "size((act2*delta_out')'), \n",
    "size((delta_out * act2')),\n",
    "size(mlp_classifer[3].W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 500), (10, 500), (10, 500))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grad_output = delta_out * act2'\n",
    "grad_input = delta_1 * act1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 3 methods)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backward(softmax::SoftMaxLayer, delta_in)\n",
    "    return delta_out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 25)(500, 25)(10, 25)"
     ]
    }
   ],
   "source": [
    "aux = forward(mlp_classifer[1], Xbatch)\n",
    "print(size(aux))\n",
    "aux = forward(mlp_classifer[2], aux)\n",
    "print(size(aux))\n",
    "aux = forward(mlp_classifer[3], aux)\n",
    "print(size(aux))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delta term for different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient computation\n",
    "\n",
    "We can use a the type hierarchy to treat diferently layers. We will now focus on  3 types of layers:\n",
    "\n",
    "- ActivationFunction\n",
    "- NeuralNetLayer \n",
    "- LossLayer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "true\n",
      "true\n",
      "false\n"
     ]
    }
   ],
   "source": [
    "println(typeof(mlp_classifer[1]) <: ActivationFunction)\n",
    "println(typeof(mlp_classifer[1]) <: NeuralNetLayer)\n",
    "\n",
    "println(typeof(mlp_classifer[2]) <: ActivationFunction)\n",
    "println(typeof(mlp_classifer[2]) <: NeuralNetLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftMaxLayer{Float64} [input_dim: 500, ouput_dim: 10]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifer[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(mlp_classifer[3]) <: ActivationFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: mlp_classifier not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: mlp_classifier not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "mlp_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_gradients (generic function with 1 method)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_gradients(mlp, loss, X, Y)\n",
    "    activations = [X]\n",
    "    signal = X\n",
    "    for (l,layer) in enumerate(mlp)\n",
    "        signal = forward(layer, signal)\n",
    "        if typeof(layer) <: ActivationFunction  \n",
    "            push!(activations, signal)\n",
    "        end\n",
    "    end\n",
    "    for (l,layer) in reverse(mlp)\n",
    "        backsignal = backward(layer, signal)\n",
    "        if typeof(layer) <: ActivationFunction  \n",
    "            push!(activations, signal)\n",
    "        end\n",
    "    end\n",
    "    return activations\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (10, 60000))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = one_hot_encoding(y_train, unique_classes, class_to_pos);\n",
    "size(X_train), size(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Float32,2},1}:\n",
       " Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]                                                                                            \n",
       " Float32[0.409869 0.508489 … 0.511245 0.548481; 0.549566 0.502302 … 0.589455 0.429273; … ; 0.440444 0.618377 … 0.419173 0.409908; 0.523728 0.596093 … 0.517019 0.494896]            \n",
       " Float32[0.0614294 0.0790431 … 0.0604551 0.0566084; 0.0895738 0.0996882 … 0.0827413 0.0943303; … ; 0.0561401 0.0575206 … 0.0509058 0.0558417; 0.119631 0.112974 … 0.107626 0.101359]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = compute_gradients(mlp_classifer,\n",
    "                        crossentropy,\n",
    "                        X_train[:,1:100],\n",
    "                        Y_train[:,1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Tuple{Int64,Int64},1}:\n",
       " (784, 100)\n",
       " (500, 100)\n",
       " (10, 100) "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size.(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  393.72 KiB\n",
       "  allocs estimate:  5\n",
       "  --------------\n",
       "  minimum time:     35.655 μs (0.00% GC)\n",
       "  median time:      165.822 μs (0.00% GC)\n",
       "  mean time:        200.527 μs (22.15% GC)\n",
       "  maximum time:     4.218 ms (90.84% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark act[2][:,1]* rand(100)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Any,1}:\n",
       " LinearLayer{Float64} [input_dim: 784, ouput_dim: 500]\n",
       " SigmoidActivation{Float32}(500)                      \n",
       " SoftMaxLayer{Float64} [input_dim: 500, ouput_dim: 10]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Int64,1}:\n",
       " 5\n",
       " 4\n",
       " 3\n",
       " 2\n",
       " 1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use the reverse function to iterate from the last layer \n",
    "# of mlp_classifier to the first one.\n",
    "\n",
    "reverse([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark gradient computation\n",
    "\n",
    "    class LinearLayer():\n",
    "        def __init__(self, num_inputs, num_units, scale=0.01):\n",
    "            self.num_units = num_units\n",
    "            self.num_inputs = num_inputs\n",
    "            self.W = np.random.random((num_inputs, num_units)) * scale\n",
    "            self.b = np.zeros(num_units)\n",
    "\n",
    "        def __str__(self): \n",
    "            return \"LinearLayer(%i, %i)\" % (self.num_inputs,\n",
    "                                            self.num_units)\n",
    "\n",
    "        def fprop(self, x, *args):\n",
    "            self.x = x\n",
    "            self.a = np.dot(x, self.W) + self.b\n",
    "            return self.a\n",
    "\n",
    "        def bprop(self, delta_in):\n",
    "            x_t = np.transpose(self.x)\n",
    "            self.grad_W = np.dot(x_t, delta_in)\n",
    "            self.grad_b = delta_in.sum(axis=0)\n",
    "            W_T = np.transpose(self.W)\n",
    "            self.delta_out = np.dot(delta_in,W_T)\n",
    "            return self.delta_out\n",
    "\n",
    "        def update_params(self, lr):\n",
    "            self.W = self.W - self.grad_W*lr\n",
    "            self.b = self.b - self.grad_b*lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 6 methods)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backward(linear_layer::LinearLayer, delta_batch_in::Array, Xbatch::Array)\n",
    "    \"\"\"\n",
    "    Given an input batch of delta terms (where the data comes as columns)\n",
    "    compute\n",
    "    \"\"\"\n",
    "    grad_W = linear_layer.W * Xbatch' \n",
    "    grad_b = delta_batch_in\n",
    "    return grad_W, grad_b\n",
    "end\n",
    "\n",
    "function forward(relu_activation::ReluActivation, Xbatch::Array)\n",
    "    return Xbatch.*( Xbatch .> 0.)\n",
    "end\n",
    "\n",
    "function forward(sigmoid_activation::SigmoidActivation, Xbatch::Array)\n",
    "    return 1./( 1 .+ exp.(-Xbatch))\n",
    "end\n",
    "\n",
    "function forward(softmax_layer::SoftMaxLayer, Xbatch::Array)\n",
    "    \"\"\"\n",
    "    Layer shrinking the output to [0,1] values.\n",
    "    Notice that sum(exp(Xbatch),1) will generate a Matrix with as many elements as\n",
    "    columns in Xbatch. \n",
    "    \"\"\"\n",
    "    Xbatch_out = softmax_layer.W * Xbatch\n",
    "    return exp.(Xbatch_out)./sum(exp.(Xbatch_out), 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (10, 60000))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = one_hot_encoding(y_train, unique_classes, class_to_pos);\n",
    "size(X_train), size(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delta (generic function with 1 method)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#backward(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mlp_classifer[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  20.51 MiB\n",
       "  allocs estimate:  39\n",
       "  --------------\n",
       "  minimum time:     563.439 ms (0.41% GC)\n",
       "  median time:      582.655 ms (0.48% GC)\n",
       "  mean time:        591.289 ms (0.40% GC)\n",
       "  maximum time:     636.607 ms (0.57% GC)\n",
       "  --------------\n",
       "  samples:          9\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark compute_gradients(mlp_classifer,\n",
    "                             crossentropy,\n",
    "                             X_train[:,1:1000],\n",
    "                             Y_train[:,1:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  2.05 MiB\n",
       "  allocs estimate:  34\n",
       "  --------------\n",
       "  minimum time:     54.938 ms (0.00% GC)\n",
       "  median time:      63.822 ms (0.00% GC)\n",
       "  mean time:        63.734 ms (0.35% GC)\n",
       "  maximum time:     68.753 ms (3.58% GC)\n",
       "  --------------\n",
       "  samples:          79\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark compute_gradients(mlp_classifer,\n",
    "                             crossentropy,\n",
    "                             X_train[:,1:100],\n",
    "                             Y_train[:,1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Float32,2},1}:\n",
       " Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]                                                                                            \n",
       " Float32[0.409869 0.508489 … 0.511245 0.548481; 0.549566 0.502302 … 0.589455 0.429273; … ; 0.440444 0.618377 … 0.419173 0.409908; 0.523728 0.596093 … 0.517019 0.494896]            \n",
       " Float32[0.0614294 0.0790431 … 0.0604551 0.0566084; 0.0895738 0.0996882 … 0.0827413 0.0943303; … ; 0.0561401 0.0575206 … 0.0509058 0.0558417; 0.119631 0.112974 … 0.107626 0.101359]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = compute_gradients(mlp_classifer,\n",
    "                        crossentropy,\n",
    "                        X_train[:,1:100],\n",
    "                        Y_train[:,1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (10, 60000), (3,))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(X_train), size(Y_train), size(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 100), (500, 100), (10, 100))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(aux[1]), size(aux[2]), size(aux[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##@time compute_gradients(mlp_classifer,\n",
    "#                        crossentropy,\n",
    "#                        X_train,\n",
    "#                        Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BLAS.vendor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Any,1}:\n",
       " LinearLayer{Float64} [input_dim: 784, ouput_dim: 500]\n",
       " SigmoidActivation{Float32}(500)                      \n",
       " SoftMaxLayer{Float64} [input_dim: 500, ouput_dim: 10]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SGD on the weights\n",
    "\n",
    "    y_probs = forward(x_batch)\n",
    "    loss = LossLayer.fprop(y_probs, target_batch)\n",
    "    losses += [loss]\n",
    "    backward(y_probs, target_batch)\n",
    "    update(learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Any,1}:\n",
       " LinearLayer{Float64} [input_dim: 784, ouput_dim: 500]\n",
       " ReluActivation{Float32}(500)                         \n",
       " SoftMaxLayer{Float64} [input_dim: 500, ouput_dim: 10]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×100 Array{Float32,2}:\n",
       " 0.0614294  0.0790431  0.0548797  …  0.0664131  0.0604551  0.0566084\n",
       " 0.0895738  0.0996882  0.088398      0.0908731  0.0827413  0.0943303\n",
       " 0.0398186  0.0402922  0.0377777     0.0349294  0.0393121  0.0346216\n",
       " 0.129657   0.125822   0.127724      0.133245   0.131919   0.125785 \n",
       " 0.0559305  0.0581756  0.0621343     0.0606674  0.0574023  0.0604792\n",
       " 0.136068   0.142551   0.138938   …  0.126478   0.14952    0.140141 \n",
       " 0.209436   0.185521   0.225227      0.227448   0.214934   0.225157 \n",
       " 0.102316   0.0984117  0.0999063     0.103221   0.105185   0.105676 \n",
       " 0.0561401  0.0575206  0.0536506     0.0546465  0.0509058  0.0558417\n",
       " 0.119631   0.112974   0.111365      0.102078   0.107626   0.101359 "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both BenchmarkTools and LossFunctions export \"params\"; uses of it in module Main must be qualified\n"
     ]
    }
   ],
   "source": [
    "aux[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implementation using BLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = Float32\n",
    "W1 = rand(T, 784, 500)\n",
    "out1, out2, out3 = zeros(T, 500), zeros(T, 1024), zeros(T, 10)\n",
    "\n",
    "BLAS.gemv!('N', T(1.0), W1, Array{Float32}(X_train[:,1]), T(0.0), out1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = Float32\n",
    "W1 = rand(T, 500, 1000)\n",
    "W2 = rand(T, 500, 500)\n",
    "W3 = rand(T, 10, 500)\n",
    "dW1, dW2, dW3 = zeros(W1), zeros(W2), zeros(W3)\n",
    "out1, out2, out3 = zeros(T, 2048), zeros(T, 1024), zeros(T, 10)\n",
    "dOut1, dOut2, dOut = zeros(T, 2048), zeros(T, 1024), zeros(T, 512 * 512)\n",
    "\n",
    "function mockNN(input::Array{Float32, 1}, error::Array{Float32, 1})\n",
    "  # Forward\n",
    "  BLAS.gemv!('N', T(1.0), W1, input, T(0.0), out1)\n",
    "  BLAS.gemv!('N', T(1.0), W2, out1, T(0.0), out2)\n",
    "  BLAS.gemv!('N', T(1.0), W3, out2, T(0.0), out3)\n",
    "\n",
    "  # Backward\n",
    "  # ∂E/∂inputs and ∂E/∂W\n",
    "  fill!(dW3, 0)\n",
    "  fill!(dOut2, 0)\n",
    "  BLAS.gemv!('N', T(1.0), W3', error, T(0.0), dOut2)\n",
    "  BLAS.ger!(T(1.0), error, out2, dW3)\n",
    "  \n",
    "  fill!(dW2, 0)\n",
    "  fill!(dOut1, 0)\n",
    "  BLAS.gemv!('N', T(1.0), W2', dOut2, T(0.0), dOut1)\n",
    "  BLAS.ger!(T(1.0), dOut2, out1, dW2)\n",
    "\n",
    "  fill!(dW1, 0)\n",
    "  fill!(dOut, 0)\n",
    "  BLAS.gemv!('N', T(1.0), W1', dOut1, T(0.0), dOut)\n",
    "  BLAS.ger!(T(1.0), dOut1, input, dW1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = rand(T, 512 * 512)\n",
    "error = rand(T, 10)\n",
    "@time mockNN(input, error)\n",
    "for i in 1:10\n",
    "  input = rand(T, 512 * 512)\n",
    "  error = rand(T, 10)\n",
    "  @time mockNN(input, error)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = Float32\n",
    "W1 = rand(T, 2048, 512 * 512)\n",
    "W2 = rand(T, 1024, 2048)\n",
    "W3 = rand(T, 10, 1024)\n",
    "dW1, dW2, dW3 = zeros(W1), zeros(W2), zeros(W3)\n",
    "out1, out2, out3 = zeros(T, 2048), zeros(T, 1024), zeros(T, 10)\n",
    "dOut1, dOut2, dOut = zeros(T, 2048), zeros(T, 1024), zeros(T, 512 * 512)\n",
    "\n",
    "function mockNN2(input::Array{Float32, 1}, error::Array{Float32, 1})\n",
    "  # Forward\n",
    "  BLAS.gemv!('N', T(1.0), W1, input, T(0.0), out1)\n",
    "  BLAS.gemv!('N', T(1.0), W2, out1, T(0.0), out2)\n",
    "  BLAS.gemv!('N', T(1.0), W3, out2, T(0.0), out3)\n",
    "\n",
    "  # Backward\n",
    "  # ∂E/∂inputs and ∂E/∂W\n",
    "  fill!(dW3, 0)\n",
    "  fill!(dOut2, 0)\n",
    "  BLAS.gemv!('T', T(1.0), W3, error, T(0.0), dOut2)\n",
    "  BLAS.ger!(T(1.0), error, out2, dW3)\n",
    "  \n",
    "  fill!(dW2, 0)\n",
    "  fill!(dOut1, 0)\n",
    "  BLAS.gemv!('T', T(1.0), W2, dOut2, T(0.0), dOut1)\n",
    "  BLAS.ger!(T(1.0), dOut2, out1, dW2)\n",
    "\n",
    "  fill!(dW1, 0)\n",
    "  fill!(dOut, 0)\n",
    "  BLAS.gemv!('T', T(1.0), W1, dOut1, T(0.0), dOut)\n",
    "  BLAS.ger!(T(1.0), dOut1, input, dW1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = rand(T, 512 * 512)\n",
    "error = rand(T, 10)\n",
    "@time mockNN(input, error)\n",
    "for i in 1:10\n",
    "  input = rand(T, 512 * 512)\n",
    "  error = rand(T, 10)\n",
    "  @time mockNN2(input, error)\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
