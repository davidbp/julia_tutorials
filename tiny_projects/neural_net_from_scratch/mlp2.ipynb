{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy NNets for education pourpouses in  Julia\n",
    "\n",
    "Interesting discussion how to make forward pass efficiently using BLAS:\n",
    "\n",
    "- https://discourse.julialang.org/t/blas-performance-issues-for-common-neural-network-patterns/565\n",
    "\n",
    "- http://int8.io/neural-networks-in-julia-hyperbolic-tangent-and-relu/\n",
    "\n",
    "- http://int8.io/backpropagation-from-scratch-in-julia-part-ii-derivation-and-implementation/\n",
    "\n",
    "\n",
    "\n",
    "#### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0],\n",
       "\n",
       "[5.0,0.0,4.0,1.0,9.0,2.0,1.0,3.0,1.0,4.0  …  9.0,2.0,9.0,5.0,1.0,8.0,3.0,5.0,6.0,8.0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = MNIST.traindata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = train[1];\n",
    "y_train = train[2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 0.0\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       " 6.0\n",
       " 7.0\n",
       " 8.0\n",
       " 9.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort(unique(train[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Linear layer and relu layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = Float32\n",
    "n_visible = 784\n",
    "n_hidden = 500\n",
    "\n",
    "srand(1234)\n",
    "W1 = randn(T, n_hidden, n_visible );\n",
    "W1 = W1/norm(W1)\n",
    "b = zeros(n_hidden);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(X_train[:,1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#linear layer\n",
    "batch = W1 * X_train[:,1:3].+b\n",
    "\n",
    "#relu\n",
    "l1_batch = batch .* (batch .>0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp(batch)./sum(exp(batch),1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Array{Float64,2}:\n",
       " 3.69209e74  3.80344e69  4.13157e48"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(exp(l1_batch),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp(l1_batch)./sum(exp(l1_batch),1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type LinearLayer{T}\n",
    "    \"\"\"\n",
    "    Standard layer between activations.\n",
    "    The output of this layer for a given input is meant to be a matrix product \n",
    "    of the input times W\n",
    "    \"\"\"\n",
    "    input_dim::Int\n",
    "    output_dim::Int\n",
    "    W::Array{T}\n",
    "    b::Vector{T}\n",
    "    seed::Int\n",
    "    \n",
    "    function LinearLayer(input, output; seed=1234)\n",
    "        srand(seed)\n",
    "        return new(input,\n",
    "                   output,\n",
    "                   randn(T,output,input)/sqrt(input),\n",
    "                   zeros(output))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearLayer{Float32}(784,500,Float32[0.0309767 0.0511305 … 0.0373952 -0.00012965; -0.0322051 -0.0588398 … 0.0261659 0.0156726; … ; -0.0580577 0.0256086 … 0.116446 -0.0452519; -0.0132269 -0.00453994 … 0.0292845 0.0139892],Float32[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],4492689440)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 784\n",
    "output_dim = 500\n",
    "l = LinearLayer{Float32}(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type ReluActivation{T}\n",
    "    \"\"\"\n",
    "    Relu Activation function latyer\n",
    "    \"\"\"\n",
    "    dim::Int\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type SoftMaxLayer{T}\n",
    "    \"\"\"\n",
    "    Standard layer between activations.\n",
    "    The output of this layer for a given input is meant to be a matrix product \n",
    "    of the input times W\n",
    "    \"\"\"\n",
    "    input_dim::Int\n",
    "    output_dim::Int\n",
    "    W::Array{T}\n",
    "    seed::Int\n",
    "    \n",
    "    function SoftMaxLayer(input, output; seed=1234)\n",
    "        srand(seed)\n",
    "        return new(input,\n",
    "                   output,\n",
    "                   randn(T,output, input)/sqrt(input))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "About Softmax layer\n",
    "\n",
    "http://stats.stackexchange.com/questions/79454/softmax-layer-in-a-neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First example\n",
    "\n",
    "Now let us define the structure, weight types (float type) of a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 500\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_classifer = [LinearLayer{Float32}(input_dim , hidden_dim),\n",
    "                 ReluActivation{Float32}(hidden_dim),\n",
    "                 SoftMaxLayer{Float32}(hidden_dim, output_dim)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write the code that accepts something like this\n",
    "# mlp(784,500,10, [\"sigmoid\", \"softmax\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Making predictions with the network\n",
    "\n",
    "We have defined a MLP as list of layers and activation functions.\n",
    "\n",
    "In order to make a prediction we need to make a forward pass through the network.\n",
    "Let us assume by now that we have a good set of weights at each layer in the network and\n",
    "we want to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 3 methods)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward(linear_layer::LinearLayer, Xbatch::Array)\n",
    "    \"\"\"\n",
    "    Given an input batch where the data comes as columns this method propagates \n",
    "    the batch using the weights of the linear layer\n",
    "    \"\"\"\n",
    "    return linear_layer.W * Xbatch .+ linear_layer.b\n",
    "end\n",
    "\n",
    "function forward(relu_activation::ReluActivation, Xbatch::Array)\n",
    "    return Xbatch.*( Xbatch .> 0.)\n",
    "end\n",
    "\n",
    "function forward(softmax_layer::SoftMaxLayer, Xbatch::Array)\n",
    "    \"\"\"\n",
    "    Layer shrinking the output to [0,1] values.\n",
    "    Notice that sum(exp(Xbatch),1) will generate a vector with as many elements as\n",
    "    columns in Xbatch. \n",
    "    \"\"\"\n",
    "    Xbatch = softmax_layer.W * Xbatch\n",
    "    return exp(Xbatch)./sum(exp(Xbatch),1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xbatch = X_train[:,1:25];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,25)(500,25)(10,25)"
     ]
    }
   ],
   "source": [
    "aux = forward(mlp_classifer[1], Xbatch)\n",
    "print(size(aux))\n",
    "aux = forward(mlp_classifer[2], aux)\n",
    "print(size(aux))\n",
    "aux = forward(mlp_classifer[3], aux)\n",
    "print(size(aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict_proba (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict_proba(mlp, Xbatch::Array)\n",
    "    for l in mlp\n",
    "        Xbatch = forward(l, Xbatch)\n",
    "    end\n",
    "    return Xbatch\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.504008 seconds (745.09 k allocations: 27.207 MB, 2.97% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×25 Array{Float64,2}:\n",
       " 5.39905e-22  1.0           9.17328e-30  …  9.1359e-32   8.48121e-20\n",
       " 2.37617e-5   1.11199e-49   2.55392e-68     5.00068e-19  2.31968e-43\n",
       " 6.93362e-51  1.8805e-97    5.01312e-33     9.6184e-80   1.53352e-83\n",
       " 3.41924e-13  1.70124e-72   8.44621e-12     1.31159e-34  4.3633e-48 \n",
       " 2.1359e-42   2.54938e-76   1.57863e-45     5.86561e-11  9.00681e-47\n",
       " 7.27582e-16  5.33328e-55   1.75377e-23  …  4.57536e-48  1.0        \n",
       " 0.000291457  1.23333e-74   1.0             1.0          1.73416e-63\n",
       " 8.83703e-17  1.10494e-125  1.52251e-36     1.17959e-45  3.82412e-82\n",
       " 1.68701e-43  4.17899e-67   3.65337e-25     2.82104e-50  3.34436e-60\n",
       " 0.999685     6.36062e-51   7.87676e-32     2.7335e-64   1.85022e-33"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each column contains a vector that represents\n",
    "# The conditional probability of the target beein from a particular class having observed\n",
    "# the input vector.\n",
    "\n",
    "@time predict_proba(mlp_classifer, Xbatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training the network\n",
    "\n",
    "Now we will deal with the learning part. That is, given a MLP architecture we will tune the weights in order to minimize some error function.\n",
    "\n",
    "\n",
    "## Cross entropy loss for mlp_classifier\n",
    "\n",
    "We will focus now on an standard loss function for classification problems. The cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[:,1:2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_classes = sort(unique(y_train));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = length(unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_classes = zeros(Int32,n_classes, length(y_train)) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_classes[:,2][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0\n",
      "2 1.0\n",
      "3 2.0\n",
      "4 3.0\n",
      "5 4.0\n",
      "6 5.0\n",
      "7 6.0\n",
      "8 7.0\n",
      "9 8.0\n",
      "10 9.0\n"
     ]
    }
   ],
   "source": [
    "for (a,b) in enumerate(unique_classes)\n",
    "    print(a, \" \", b, \"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Float64,Int64} with 10 entries:\n",
       "  0.0 => 1\n",
       "  4.0 => 5\n",
       "  7.0 => 8\n",
       "  9.0 => 10\n",
       "  2.0 => 3\n",
       "  3.0 => 4\n",
       "  5.0 => 6\n",
       "  8.0 => 9\n",
       "  6.0 => 7\n",
       "  1.0 => 2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_pos = Dict(class =>pos for (pos,class) in enumerate(unique_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_hot_encoding (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function one_hot_encoding(y_train)\n",
    "    unique_classes = sort(unique(y_train))\n",
    "    class_to_pos = Dict(class =>pos for (pos,class) in enumerate(unique_classes))    \n",
    "    encoded_classes = zeros(length(unique_classes), length(y_train))\n",
    "    for (i,y) in enumerate(y_train)\n",
    "        encoded_classes[:,i][class_to_pos[y]] = 1.\n",
    "    end\n",
    "    return encoded_classes\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = one_hot_encoding(y_train)[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: incomplete: premature end of input",
     "output_type": "error",
     "traceback": [
      "syntax: incomplete: premature end of input",
      ""
     ]
    }
   ],
   "source": [
    "type linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = Float32\n",
    "W1 = rand(T, 500, 1000)\n",
    "W2 = rand(T, 500, 500)\n",
    "W3 = rand(T, 10, 500)\n",
    "dW1, dW2, dW3 = zeros(W1), zeros(W2), zeros(W3)\n",
    "out1, out2, out3 = zeros(T, 2048), zeros(T, 1024), zeros(T, 10)\n",
    "dOut1, dOut2, dOut = zeros(T, 2048), zeros(T, 1024), zeros(T, 512 * 512)\n",
    "\n",
    "function mockNN(input::Array{Float32, 1}, error::Array{Float32, 1})\n",
    "  # Forward\n",
    "  BLAS.gemv!('N', T(1.0), W1, input, T(0.0), out1)\n",
    "  BLAS.gemv!('N', T(1.0), W2, out1, T(0.0), out2)\n",
    "  BLAS.gemv!('N', T(1.0), W3, out2, T(0.0), out3)\n",
    "\n",
    "  # Backward\n",
    "  # ∂E/∂inputs and ∂E/∂W\n",
    "  fill!(dW3, 0)\n",
    "  fill!(dOut2, 0)\n",
    "  BLAS.gemv!('N', T(1.0), W3', error, T(0.0), dOut2)\n",
    "  BLAS.ger!(T(1.0), error, out2, dW3)\n",
    "  \n",
    "  fill!(dW2, 0)\n",
    "  fill!(dOut1, 0)\n",
    "  BLAS.gemv!('N', T(1.0), W2', dOut2, T(0.0), dOut1)\n",
    "  BLAS.ger!(T(1.0), dOut2, out1, dW2)\n",
    "\n",
    "  fill!(dW1, 0)\n",
    "  fill!(dOut, 0)\n",
    "  BLAS.gemv!('N', T(1.0), W1', dOut1, T(0.0), dOut)\n",
    "  BLAS.ger!(T(1.0), dOut1, input, dW1)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "input = rand(T, 512 * 512)\n",
    "error = rand(T, 10)\n",
    "@time mockNN(input, error)\n",
    "for i in 1:10\n",
    "  input = rand(T, 512 * 512)\n",
    "  error = rand(T, 10)\n",
    "  @time mockNN(input, error)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = Float32\n",
    "W1 = rand(T, 2048, 512 * 512)\n",
    "W2 = rand(T, 1024, 2048)\n",
    "W3 = rand(T, 10, 1024)\n",
    "dW1, dW2, dW3 = zeros(W1), zeros(W2), zeros(W3)\n",
    "out1, out2, out3 = zeros(T, 2048), zeros(T, 1024), zeros(T, 10)\n",
    "dOut1, dOut2, dOut = zeros(T, 2048), zeros(T, 1024), zeros(T, 512 * 512)\n",
    "\n",
    "function mockNN2(input::Array{Float32, 1}, error::Array{Float32, 1})\n",
    "  # Forward\n",
    "  BLAS.gemv!('N', T(1.0), W1, input, T(0.0), out1)\n",
    "  BLAS.gemv!('N', T(1.0), W2, out1, T(0.0), out2)\n",
    "  BLAS.gemv!('N', T(1.0), W3, out2, T(0.0), out3)\n",
    "\n",
    "  # Backward\n",
    "  # ∂E/∂inputs and ∂E/∂W\n",
    "  fill!(dW3, 0)\n",
    "  fill!(dOut2, 0)\n",
    "  BLAS.gemv!('T', T(1.0), W3, error, T(0.0), dOut2)\n",
    "  BLAS.ger!(T(1.0), error, out2, dW3)\n",
    "  \n",
    "  fill!(dW2, 0)\n",
    "  fill!(dOut1, 0)\n",
    "  BLAS.gemv!('T', T(1.0), W2, dOut2, T(0.0), dOut1)\n",
    "  BLAS.ger!(T(1.0), dOut2, out1, dW2)\n",
    "\n",
    "  fill!(dW1, 0)\n",
    "  fill!(dOut, 0)\n",
    "  BLAS.gemv!('T', T(1.0), W1, dOut1, T(0.0), dOut)\n",
    "  BLAS.ger!(T(1.0), dOut1, input, dW1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = rand(T, 512 * 512)\n",
    "error = rand(T, 10)\n",
    "@time mockNN(input, error)\n",
    "for i in 1:10\n",
    "  input = rand(T, 512 * 512)\n",
    "  error = rand(T, 10)\n",
    "  @time mockNN2(input, error)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
